---
layout: post
comments: true
title: Spark란?
categories: Spark

tags:
- Spark
---


**<span style='color:DarkRed'>Spark 특징</span>**

- Spark는 RDD(Resilient/Distributed/Dataset)객체를 사용하여 연산이 수행됨
	- ```Spark context```함수를 이용하여 데이터셋을 RDD로 캡슐화(Input $\rightarrow$ RDD1)
	- 초기 RDD는 다른 RDD로 recurrent하게 연산이 연결됨(<span style='color:green'>직접적인 연산이 수행되는 것이 아니라 연산방법만 기억함</span>) = lineage of the **deterministic** operation (RDD1$\rightarrow$ RDD2(split operation) $\rightarrow$ RDD3(selective operation)...))
	- RDD = immutable dataset
	- <span style='color:green'>결과를 미리 수행하지 않기 때문에</span>, RDD partition이 손실이 일어나더라도 다시 계산할 수 있음(fault-tolerant dataset)
	- 캡슐화되어 있는 RDD는 action함수로 결과를 출력할 수 있음(e.g. take, collect, count, ...)


<br>        
        
- Source Languages
    - scala, python, java
    - scala를 사용하는 이유
        - Spark는 scala로 쓰여져 있음
        - Scala funtion들이 분산처리하기 좋게 되어 있음
        - Simple code
        - 비교적 python은 느림 
    - Scala에 익숙치 않다면, <a href="https://github.com/Donghwa-KIM/Spark-scala-jupyter-tutorial/blob/master/00_scala_tutorial.ipynb">Scala tutorial</a>을 참고

<br>

- Scala는 Python과 유사한 문법을 가짐

    - Python

    ```python
    nums = sc.parallelize([1,2,3,4])
    squared = list(nums.map(lambda x: x*x))
    ```

    - Scala
    
    ```scala
    val nums = sc.parallelize(List(1,2,3,4))
    val squared = nums.map(x => x*x).collect()
    ```



<br>

**<span style='color:DarkRed'>Spark Application </span>**

- Spark Streaming
    - R(Resilient:회복력)D(Distributed)D(Dataset) 처리/변환/수집
    - 실시간으로 작은 배치 데이터를 처리
- Spark SQL
    - 대용량 데이터에 query를 사용할 때 사용
- MLLib
    - 대용량 데이터를 분산처리하여 머신러닝 학습
- GraphX
    - 그래프 네트워크
    - Social network
    - pregel(프레글) for BFS search
