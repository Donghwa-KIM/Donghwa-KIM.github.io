---
layout: post
comments: true
title: Prior & Posterior Predictive Distributions
categories: Bayesian Statistic(베이지안 통계)
tags:
- Bayesian Statistic(베이지안 통계)
---


**<span style='color:DarkRed'>Observed Case </span>**

 - Prior distribution($p(\theta)$): 데이터를 알기 전에 분포
 - Posterior distribution($p(\theta \| y)$): 데이터를 알고 난 후의 분포

> [앞서]({{ site.url }}/baye_01.html) 배운 위의 분포들은 현재 학습데이터에 대한 분포로, unobserved data에 대해서는 다른 명칭으로 정의한다.


<br>

**<span style='color:DarkRed'>Unobserved Case</span>**


> 앞에서 배운 베이지안 개념을 가지고, unobserved data($\tilde{y}$)에 대해서 예측하는 분포를 생각해보자. $\theta$ 대신에 $y$ (observed)가 conditional하게 들어간 것을 주의하자.
>
> bayes rule을 이용해서 unobserved data($\tilde{y}$)의 확률을 구하면,

- **<span style='color:DarkBlue'>Prior Predictive Distributions</span>**





$$ p(\tilde{y}) = \int p(\tilde{y}, \theta) d\theta = \int p( \tilde{y}| \theta) \times p(\theta) d\theta$$



- **<span style='color:DarkBlue'>Posterior Predictive Distributions</span>**

$$ p(\tilde{y}| y) = \int p(\tilde{y}, \theta | y) d\theta = \int p( \tilde{y}| \theta, y) \times p(\theta| y) d\theta$$

> 일반적으로 $y$ 와 $\tilde{y}$는(i.i.d) 독립이라고 가정하기 때문에, baye rule로 전개하면, likelihood(for unobserved data)와 posterior(for observed data)의 곱으로 정의된다.
> 
$$ p(\tilde{y}| y) = \int p(\tilde{y}, \theta | y) d\theta = \int p( \tilde{y}| \theta) \times p(\theta| y) d\theta$$

