---
layout: post
comments: true
title: Binomial distribution
categories: Bayesian Statistic(베이지안 통계)
tags:
- Bayesian Statistic(베이지안 통계)
---

**<span style='color:DarkRed'>Binomial 예제</span>**

Each female of age 65 or over in the 1998 General Social Survey was asked whether or not they were generally happy. Let $Y_i = 1$ if respondent i reported being generally happy, and let $Y_i = 0$ otherwise. If we lack information distinguishing these $n = 129$ individuals we may treat their responses as being exchangeable.

> - 129명의 실험자들을 Y에 대해서 평가하는 설문조사를 실시
> - $Y_i = 0:$ happy
> - $Y_i = 1:$ unhappy
> - $Y_i$ given $\theta$는 i.i.d를 만족

- y는 bernoulli 특성에 따라 아래와 같이 정의할 수 있다. (likelihood 관점)

$$
\begin{align}
p(y_1,...,y_{129} | \theta) &= p(y_1| \theta)\times \cdots \times p(y_{129} | \theta) \quad \because i.i.d\\
&= \theta^{\sum^{129}_{i=1} y_i}(1-\theta)^{129-\sum^{129}_{i=1}y_i} 
\end{align}
$$

<br>

- $\theta$가 $[0,1]$의 임의의 값(uniform = $\frac{1}{b-a}$)을 가진다고 가정해보자.

$$p(\theta) = 1 \text{  for all  } \theta \in [0,1]$$ 

<br>


- posterior를 bayes' rule를 사용해서 전개해보면 다음과 같다.

$$p(\theta | y_1,...,y_{129}) = \frac{p(y_1,...,y_{129} | \theta)p(\theta)}{p(y_1,...,y_{129})}$$

$$p(\theta | y_1,...,y_{129}) \propto p(y_1,...,y_{129} | \theta)p(\theta)$$

$$p(\theta | y_1,...,y_{129}) \propto p(y_1,...,y_{129} | \theta)$$

$$p(\theta | y_1,...,y_{129}) \propto p(y_1,...,y_{129} | \theta)$$

<br>

- 129명의 실험결과가 다음과 같다면, 
	- 118명(91$\%$): happy
	- 11명(9$\%$): unhappy
	- $p(y_1,...,y_{129} \| \theta) = \theta^{118}(1-\theta)^{11}, \because \text{likelihood} \sim \text{binomial}$


	- <span style='color:Blue'>$p(\theta \| y_1,...,y_{129})$</span> $= \theta^{118}(1-\theta)^{11} \times p(\theta) / p(y_1,...,y_{129}) = \theta^{118}(1-\theta)^{11} \times 1 /$ <span style='color:DarkRed'>$p(y_1,...,y_{129}) $</span>

<br>

- 정확하게 posterior를 구하기 위해 <span style='color:DarkRed'>$p(y_1,...,y_{129}) $</span>를 구해보자.

<p align="center">
$ 1 = \int^{1}_{0}$ <span style='color:Blue'>$p(\theta | y_1,...,y_{129})$</span> $d\theta $</p>

<p align="center">
$ 1 = \int^{1}_{0} \theta^{118}(1-\theta)^{11} \times 1 / $ <span style='color:DarkRed'>$p(y_1,...,y_{129}) $</span> $d\theta $ </p>

$$ 1 = \frac{1}{p(y_1,...,y_{129})} \int^{1}_{0} \theta^{118}(1-\theta)^{11}  d\theta $$ 

<br>


- [[Proof]]({{ site.url }}/ref_Gamma_beta.html)

$$ \because \int^{1}_{0} \theta^{118}(1-\theta)^{11}   d\theta  = \frac{\Gamma(119)\Gamma(12)}{\Gamma(131)}$$ 

<br>

- 대입하면,

$$ 1 = \frac{1}{p(y_1,...,y_{129})} \frac{\Gamma(119)\Gamma(12)}{\Gamma(131)} $$ 

$$ p(y_1,...,y_{129})= \frac{\Gamma(119)\Gamma(12)}{\Gamma(131)} $$ 

<br>

- 위 식을 posterior에 대입하고 정리하면, $beta(119,12)$ 분포를 따른다는 것을 알 수 있다.

$$p(\theta | y_1,...,y_{129}) = \frac{\Gamma(119)\Gamma(12)}{\Gamma(131)} \theta^{118}(1-\theta)^{11}$$

$$ beta(119,12)= \frac{\Gamma(119)\Gamma(12)}{\Gamma(131)} \theta^{119-1}(1-\theta)^{12-1}$$

$$\because beta(a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}$$

<br>


**<span style='color:DarkRed'>Beta분포의 평균과 분산</span>**


- Beta 분포는 아래와 그림같이 두개의 인자로 control되는 분포를 가진다.

<p align="center"><img width="500" height="auto" src="https://imgur.com/tnG0Dqp.png"></p>

<br>

- **beta분포 평균**

$$\mu = \operatorname{E}[X]
     = \int_0^1 x f(x;\alpha,\beta)\,dx$$

$$     = \int_0^1 x\frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}\,dx = \dfrac{\int_0^1 x^{\alpha} (1-x)^{\beta-1}\ dx}{B(\alpha,\beta)}= B(\alpha+1,\beta)\times \dfrac{1}{B(\alpha,\beta)} $$

$$ \because \Gamma(n+1) = n\Gamma(n)$$

$$= \dfrac{\Gamma(\alpha+1) \Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} = \dfrac{\alpha \Gamma(\alpha) \Gamma(\beta)}{(\alpha+\beta)\Gamma(\alpha+\beta)} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} $$

$$= \dfrac{\alpha}{\alpha+\beta}$$

<br>

- **beta분포 분산**


$$\mu^2 + \sigma^2 = \operatorname{E}[X^2]
     = \int_0^1 x^2 f(x;\alpha,\beta)\,dx$$

$$     = \int_0^1 x^2 \frac{x^{\alpha-1}(1-x)^{\beta-1}}{Beta(\alpha,\beta)}\,dx = \dfrac{\int_0^1 x^{\alpha+1} (1-x)^{\beta-1}\ dx}{B(\alpha,\beta)}= B(\alpha+2,\beta)\dfrac{1}{B(\alpha,\beta)} $$

$$= \dfrac{\Gamma(\alpha+2) \Gamma(\beta)}{\Gamma(\alpha+\beta+2)} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} = \dfrac{(\alpha+1) \Gamma(\alpha+1) \Gamma(\beta)}{(\alpha+\beta+1)\Gamma(\alpha+\beta+1)} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} $$

$$ = \dfrac{(\alpha+1) \alpha \Gamma(\alpha) \Gamma(\beta)}{(\alpha+\beta+1)(\alpha+\beta)\Gamma(\alpha+\beta)} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} = \dfrac{\alpha(\alpha+1)}{(\alpha+\beta+1)(\alpha+\beta)}$$

$$ \sigma^2 = \dfrac{\alpha(\alpha+1)}{(\alpha+\beta+1)(\alpha+\beta)} - \dfrac{\alpha^2}{(\alpha+\beta)^2}$$

$$ =\dfrac{\alpha}{\alpha+\beta} \Biggl\{ \dfrac{\alpha^2 + \alpha + \beta + \alpha \beta - \alpha^2 -\alpha \beta - \alpha}{(\alpha+\beta)(\alpha+\beta+1)} \Biggr\}$$

$$ =\dfrac{\alpha}{\alpha+\beta} \times \dfrac{\beta}{\alpha+\beta} \times \dfrac{1}{\alpha+\beta+1}$$

<br>

**<span style='color:DarkRed'>Beta prior example</span>**

- $\theta \sim beta(a,b)$
- $Y\|\theta \sim binomial(n,\theta)$


$$
\begin{align}
p(\theta | y) &= \dfrac{p(\theta)p(y|\theta)}{p(y)}\\
& = \dfrac{1}{p(y)} \times \dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1} \times {n\choose y}\theta^{y}(1-\theta)^{n-y}\\
& = c(n,y,a,b) \times \theta^{a+y-1}(1-\theta)^{b+n-y-1}\\
& = beta(a+y,b+n-y)\\
& = beta(a+\sum^{n}_{i=1}y_i,\ b+n-\sum^{n}_{i=1}y_i)\\
\end{align} 
$$

<br>


- $\theta \|Y$ 의 expection을 구하기 위해 beta($\alpha$,$\beta$)분포 평균($\dfrac{\alpha}{\alpha+\beta}$)을 구하는 식을 이용하면,

$$ \theta | Y \sim beta(a+\sum^{n}_{i=1}y_i,\ b+n-\sum^{n}_{i=1}y_i)$$


$$
\begin{align}
E(\theta | y) & = \dfrac{\alpha}{\alpha+\beta}\\
&= \dfrac{a+\sum^{n}_{i=1}y_i}{ a+\sum^{n}_{i=1}y_i+b+n-\sum^{n}_{i=1}y_i}\\
&= \dfrac{a+\sum^{n}_{i=1}y_i}{ a+b+n} \\
&= \dfrac{a}{ a+b+n} + \dfrac{\sum^{n}_{i=1}y_i}{a+b+n}\\
&= \dfrac{a+b}{ a+b+n} \times \dfrac{a}{a+b}  + \dfrac{n}{a+b+n} \times \dfrac{\sum^{n}_{i=1}y_i}{n}\\
&= w_1\times \dfrac{a}{a+b}  +w_2\times \dfrac{\sum^{n}_{i=1}y_i}{n}\\
&= w_1\times \text{prior mean info}  +w_2\times \text{sample average info}\\
\end{align} 
$$

> 지금까지 $\theta$의 prior분포를 beta분포로 가정하여 posterior 전개 하였다. 그런데 이 posterior는 sample average(observed data)와 prior mean(prior belief)의 가중합으로 이루어 진것을 확인할 수 있다. 
> -	만약 샘플수가 충분히 많다면, $w_2 \rightarrow 1$, $w_1 \rightarrow 0$ 으로 수렴할 것이다.(prior 무시)
> - 샘플수(데이터)가 어느정도 있다면 observed data와 prior belief의 가중평균이 될 것이다.
> - 샘플수가 거의 없다면, $w_2 \rightarrow 0$, $w_1 \rightarrow 1$ 으로 수렴할 것이다.(prior만 고려)

<br>

**<span style='color:DarkRed'>Conjugate prior</span>**

- 위 예제에서 $\theta$의 **beta** prior distribution이 동일한 **beta** posterior distribution($ p(\theta \|y)$)으로 표현될 때, 그 prior(beta)를 binomial likelihood($p(y \|\theta)$)의 conjugate prior 라고 명칭한다.

<br>

**<span style='color:DarkRed'>Prediction</span>**

- 새로운 데이터에 대해서 $\tilde{Y} = 1$일 posterior predictive distribution을 구해보자.
- 결과적으로, likelihood를 binomial로 가정하면 예측확률은 $\theta$ Posteior의 기대값이 된다.
$$
\begin{align}
p(\tilde{Y}=1 | y_1,...,y_n)& = \int p(\tilde{Y}=1,\theta|y_1,...y_n) d\theta\\
& = \int p(\tilde{Y}=1|\theta, y_1,...y_n)\times p(\theta | y_1,...y_n) d\theta\\
& = \int p(\tilde{Y}=1|\theta)\times p(\theta | y_1,...y_n) d\theta, \ (\because y_{i} \sim iid)\\
& = \int {1 \choose 1} \theta^{1}(1-\theta)^{1-1} \times p(\theta | y_1,...y_n) d\theta \\
& = \int \theta \times p(\theta | y_1,...y_n) d\theta = \mathbb{E}[ \theta| y_1,...y_n ] \\
& = \dfrac{a+b}{ a+b+n} \times \dfrac{a}{a+b}  + \dfrac{n}{a+b+n} \times \dfrac{\sum^{n}_{i=1}y_i}{n}\\ 
\end{align} 
$$

- 예제
	- The uniform prior distribution, or beta(1,1) prior

$$
\begin{align}
Pr(\tilde{y}=1|Y=y) & = \mathbb{E}[\theta|Y=y]\\
& = \dfrac{a+b}{ a+b+n} \times \dfrac{a}{a+b}  + \dfrac{n}{a+b+n} \times \dfrac{\sum^{n}_{i=1}y_i}{n}\\
& = \dfrac{1+1}{ 1+1+n} \times \dfrac{1}{1+1}  + \dfrac{n}{1+1+n} \times \dfrac{\sum^{n}_{i=1}y_i}{n}\\
& = \dfrac{2}{ 2+n} \times \dfrac{1}{2}  + \dfrac{n}{2+n} \times \dfrac{\sum^{n}_{i=1}y_i}{n}\\
\end{align} 
$$
