---
layout: post
comments: true
title: 베이지안 통계란?
categories: Bayesian Statistic(베이지안 통계)
tags:
- Bayesian Statistic(베이지안 통계)
---


**<span style='color:DarkRed'>베이지 통계의 정의</span>**

- Frequency probability(Relative Frequency): 관찰되는 데이터를 기반으로 반복되는 특정 결과의 기대값(sample average)을 정의
	- e.g. 동전을 계속 던지다 보면 "앞면"이 나올 확률은 1/2
- Subjective probability(Bayesian statistics): 특정 결과의 일어날 확률이 개개인의 판단(믿음의 정도)에 의해 정해지는 것
	- e.g. 같은 조건에서도 사람들마다 약속장소에 도착할 확률은 개개인에 따라서 달라짐

<br>
**<span style='color:DarkRed'>베이지 통계의 예제</span>**

<p align="center"><img width="200" height="auto" src="https://imgur.com/zuFEKGJ
.png"></p>
- 순차적으로 두개의 당구공을 무작위로 테이블에 던지는 상황을 생각해보자.

<p align="center"><img width="500" height="auto" src="https://imgur.com/byhKRRz.png"></p>

> - $Y$: 두번째공이 첫번째공의 왼쪽/오른쪽인지를 나타내는 확률변수 $\sim$ Bernoulli distribution
> - $\theta$: 첫번째공의 위치, 랜덤하게 [0,1]의 무작위 값을 가진다고 가정
> - first: 첫번째 공의 위치(아래의 그림에서 화살표 끝 부분)
> - second: 두번째 공의 위치(아래의 그림에서 화살표 끝 부분)

- 두번째공이 첫번째 공의 <span style='color:Red'>왼쪽($Y=0$)</span>/<span style='color:Blue'>오른쪽($Y=1$)</span>에 있을 때, 첫번째 공의 위치에 대한 확률을 구해보자.
- 먼저 위 문제를 posterior $\propto$ likelihood $\times$ prior로 해석해보자.

> - prior: 데이터가 **관측되기 전**에 $\theta$(우리가 찾고자 하는 것)의 확률
> 	- 확률이 동등하므로 단순하게 uniform 분포를 가정
>	- $p(\theta) = p(\text{parameter}) = 1 $
> 
> - posterior: 데이터가 **관측된 후**에 $\theta$의 확률
> 	- 위 예제에서, 두번째공이 <span style='color:Red'>왼쪽</span>에 있을수록, $\theta$가 큰값(오른쪽)일 확률이 커짐  
> 	- 위 예제에서, 두번째공이 <span style='color:Blue'>오른쪽</span>에 있을수록, $\theta$가 작은값(왼쪽)일 확률이 커짐  
>	- $p(\theta \| Y)= p(\text{parameter} \| \text{data})$
> - likelihood: $\theta$ 주어 졌을 때, 데이터가 발생할 확률
> 	- 위 예제에서, $\theta$가 왼쪽에 있을수록, 두번째공이 <span style='color:Blue'>오른쪽($Y=1$)</span>일 확률이 커짐  
> 	- 위 예제에서, $\theta$가 오른쪽에 있을수록, 두번째공이 <span style='color:Red'>왼쪽($Y=0$)</span>일 확률이 커짐
> 	- $p(Y \| \theta)= p(\text{data} \| \text{parameter})$ 
> 	- $Y$는 0 or 1 이기 때문에, 베르누이 분포를 가정
> 		- <span style='color:Red'>빨간색($Y=0$)</span> 길이: $p(Y \| \theta) = \theta,$  if <span style='color:Red'>$Y=0$</span> 
> 		- <span style='color:Blue'>파란색($Y=1$)</span> 길이: $p(Y \| \theta) = 1- \theta,$ if <span style='color:Blue'>$Y=1$</span> 
>		- $p(Y \| \theta) =\theta^{1-Y}(1-\theta)^{Y}$

- baye rule로 사용하면 다음과 같이 분해할 수 있음
	- $p(Y) = \frac{1}{2}$ (Y는 0 or 1)
	-  $p(\theta \| Y)=  \frac{p(Y \| \theta)\times p(\theta)}{p(Y)} =  \frac{likelihood \times prior}{evidence} = \frac{\theta^{1-Y}(1-\theta)^{Y} \times 1}{ 1/2}$
	- $p(\theta \| Y) = 2\theta,$  if <span style='color:Red'>$Y=0$</span>
	- $p(\theta \| Y) = 2(1-\theta),$  if <span style='color:Blue'>$Y=1$</span>

<br>

- 위에서 도출한 식을 plot하면 아래와 같다.

<p align="center"><img width="400" height="auto" src="https://imgur.com/GpANjI5.png"></p>

> - 초기의 Posterior는 prior(p($\theta$))와 같다고 할 수 있으므로, 그림에서 1로 점선 처리된 것을 확인 할 수가 있다.
> - 여기에 Y를 conditional하게 주게되면 prior는 기울기를 가지는 직선의 posterior(p($\theta \| Y$))로 업데이트 된다고 할 수 있다.

- 그럼 좀 더 일반화해서 생각해 보자.


<p align="center"><img width="600" height="auto" src="https://imgur.com/0JImimu.png"></p>

> - 왼쪽그림은 prior 분포를 가우시안 분포를 가정하였고, $\mu = 10$, $\sigma = 0.01$ 가정
> - 오른쪽그림은 prior 분포를 가우시안 분포를 가정하였고, $\mu = 10$, $\sigma = 0.5$ 가정

먼저 알 수 있는 것은,
-  데이터가 많아 질수록 prior에 관계없이 posterior distribution가 true distribution에 가까워진다.
- 약간의 데이터가 존재할 경우, prior의 영향력은 커지며, 평균이 10으로 고정 되어있을 때 prior의 분산을 크게한 경우가 분산을 적게 한 것 보다 actual mean 쪽에서의 prior값이 상대적으로 크기 때문에 결과적으로 posterior $\propto$ likelihood $\times$ prior가 true distribution에 가까워진다.