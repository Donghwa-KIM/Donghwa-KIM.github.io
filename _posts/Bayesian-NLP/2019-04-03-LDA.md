---
layout: post
comments: true
title: LDA
categories: Bayesian NLP
tags:
- Bayesian NLP
---



**<span style='color:DarkRed'>LDA로 보는 베이지안 </span>**

<p align="center"><img width="400" height="auto" src="https://i.imgur.com/K9stwbW.png"></p>

- $K$: 토픽갯수
- $M$: 문서의 수
- $N$: 각 문서에 담긴 단어의 수 
- $W$: 어느 한 문서의 단어
- $Z$: 토픽을 표현하는 잠재 벡터
  - deterministic한 고정된 vector가 아니라 특정 분포($\theta$)에서 샘플링된 vector라고 생각 할 수 있다.
  - 특정 ```latent space```(topic)를 찾은 다음 원하는 target(단어)분포을 예측하기 때문에 ```generative model```이라고 할 수 있다.
- Plate notation
  - ```Plate```는 random variable의 집합이라고 할 수 있다.
  - ```grapical model```에서는 각 노드(circle)들을 ```random variable```로 가정하지만, LDA에서 고정된 ```hyperparameter```(random variable이 아닌) $\alpha$와 $\beta$로 표현하기도 한다.
  - $\beta$가 있는 ```block```안에 $K$의 의미는 $\beta$가  $K$개 있다는 것을 ```plate```표현한 것이다.
      - $\beta_1, \beta_2, ..., \beta_K$
  - $\theta \rightarrow Z \rightarrow W$는 문서 $M$번 만큼 수행된다.
  - 특 한 문서의 $Z \rightarrow W$는 그 문서의 단어 $N$번만큼 수행된다.

- Joint distribution

$$
\begin{align}
\definecolor{red}{RGB}{255,0,0}
\definecolor{green}{RGB}{0,255,0}
\definecolor{blue}{RGB}{0,0,255}
\definecolor{energy}{RGB}{114,0,172}
\definecolor{freq}{RGB}{45,177,93}
\definecolor{spin}{RGB}{251,0,29}
\definecolor{signal}{RGB}{18,110,213}
\definecolor{circle}{RGB}{217,86,16}
\definecolor{average}{RGB}{203,23,206}
\prod^{M}_{j=1}p(W_{1}^{(j)},...,W_{n}^{(j)},Z^{(j)}_{i},..., Z^{(j)}_{N}, \theta^{(j)}|\beta_1, ..., \beta_{K}, \alpha)\\
= \prod^{M}_{j=1}\bigg(p(\theta^{(j)}|\alpha)\prod^{n}_{i=1}p(Z^{(j)}_{i}|\theta)p(W^{(j)}_{i}|Z^{(j)}_{i}, \theta^{(j)},\beta_{1},...,\beta_{K})\bigg)
\end{align}
$$


