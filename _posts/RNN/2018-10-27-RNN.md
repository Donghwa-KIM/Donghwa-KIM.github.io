---
layout: post
comments: true
title: RNN(Recurrent Neural Network)
categories: RNN

tags:
- RNN
---


**<span style='color:DarkRed'>Related work</span>**

```A fixed-window	neural Language Model```은 **고정된 작은 window size**를 사용한다는 점과 학습되는 $\mathbf{W}_h$ **weight matrix가 공유되지 않는다는 점**에서 한 대안으로 RNN을 제시하였다.

<br>

**<span style='color:DarkRed'>RNN</span>**
- 먼저 RNN의 구조를 살펴보자.
<p align="center"><img width="500" height="auto" src="https://imgur.com/hRIrv9P.png"></p>
- $one$-hot vector로 표현된 t시점의 하나의 word를 $\mathbf{x}^{(t)} \in \mathbb{R}^{\|V\|}$라고 표현해보자.
	- $V$: vocabulary size
	- $\mathbf{x}^{(t)} = \[0,0,...1,0,0\]^{T}$
	- len$(\mathbf{x}^{(t)}) = \|V\|$
- 아래의 그림을 예시로, 각 word들을 word2vec 모델($f(x)$)을 활용하여, 4차원으로 임베딩한것을 $\mathbf{e}^{(t)}$라고 하자.  
	- $f(\mathbf{x}^{(t)}) = \mathbf{e}^{t}$ , 아래의 그림으로 해석하면  $\mathbf{E}\mathbf{x}^{(t)} = \mathbf{e}^{t} $
- 현재 hidden state는($\mathbf{h}^{t}$)는 이전 hidden state($\mathbf{h}^{t-1}$)와 현재 embedded input vector($\mathbf{e}^{t}$)의 선형결합(with bias $\mathbf{b}_{hidden}$)의 activation($\sigma$)을 취한 벡터라고 할 수 있다.  
	- $\mathbf{h}^{t} = \sigma(\mathbf{W}_h \mathbf{h}^{t-1}+ \mathbf{W}_e\mathbf{e}^{t}+\mathbf{b}_h)$
	- 위 그림에서, 
		- $\mathbf{h}^{t} \in \mathbb{R}^4$
		- $\mathbf{W}_{h} \in \mathbb{R}^{4 \times 4}$	
		- $\mathbf{h}^{t-1} \in \mathbb{R}^4$
		- $\mathbf{W}_{e} \in \mathbb{R}^{4 \times 4}$
		- $\mathbf{e}^{t} \in \mathbb{R}^{4}$
		- $\mathbf{b}_{hidden} \in \mathbb{R}^4$	
- 만약 우리가 ```the``` ```students``` ```opened``` ```their```이후 다음으로 등장한 단어 $\mathbf{x}^{(5)}$를 찾는다면, 그전에 마지막 hidden state($\mathbf{h}^{(4)}$)를 이용하여 실제단어 $\mathbf{x}^{(5)}$의 확률을 최대화하는 예측단어 $\hat{\mathbf{y}}^{(4)}$ 예측할 수 있다.
	- $\hat{\mathbf{y}}^{(t)} = \text{softmax}(\mathbf{U}\mathbf{h}^{(t)}+b_{output}) $ 
	- 위 그림에서,
		- $\hat{\mathbf{y}}^{(t)} \in \mathbb{R}^{\|V\|}$
		- $\mathbf{U} \in \mathbb{R}^{\|V\| \times 4}$
		- $\mathbf{h}^{t} \in \mathbb{R}^{4}$
		- $\mathbf{b}_{output} \in \mathbb{R}^{\|V\|}$

<br>
**<span style='color:DarkRed'>Loss function</span>**
- 각 $t$-step에 대한 loss는 아래와 그림 같이 표현된다.
	- 모델은 다음의 단어의 확률이 최대화되는 $\theta$를 찾는 것이 목적이며, negative log-liklihood를 최소화하는 것과 같다.
	- 그리고, 목표값은 0과 1로 이루어진 multi-class로 ```target distribution```($y_j$)을 ```multinomial distribution```을 가정하게 된다.
	- 이렇게 ```multinomial distribution```을 가정하게 되면, negative log-liklihood는 아래와 같은 cross-entropy의 형태로 사용될 수 있다.

$$
\begin{align}
\hat{y}_{j} &= \dfrac{e^{f_j}}{\sum_{i} e^{f_i}}, \quad y_j \sim \text{Multinomial}(\hat{y}_{1},\hat{y}_{2}, \cdots)\\
P(Y_1=y_1 \text{ and } Y_2=y_2 \ \cdots)  &= \begin{cases} { \displaystyle {n! \over y_1!\cdots y_{|V|}!}\hat{y}_1^{y_1}\times\cdots\times \hat{y}_{|V|}^{y_{|V|}}}, \quad &
\text{when } \sum_{i=1}^{|V|} y_i=n=1 \\  \\
0 & \text{otherwise,} \end{cases}\\
 &= \prod^{|V|}_{j=1}\hat{y}_{j}^{y_j} \quad \text{for } y_j\in\{0,1\}\\
L(p) &= -\log P(Y_1=y_1 \text{ and } Y_2=y_2 \ \cdots)  \\
L(p) &= -\log\prod^{|V|}_{j=1} \hat{y}_{j}^{y_j} \\
\end{align}
$$

$$ \therefore J^{(t)}(\theta)=\text{CE}(\mathbf{y}^{(t)},\hat{\mathbf{y}}^{(t)})= -\sum^{| V |}_{j=1}y^{(t)}_j \log\hat{y}_{j}^{(t)}$$

- 총 loss는 각 step의 평균값을 가진다.

$$J(\theta) = \frac{1}{T}\sum^{T}_{t=1} J^{(t)}(\theta)$$


<p align="center"><img width="600" height="auto" src="https://imgur.com/MIEKyZD.png"></p>

- batch size가 1이상 일 경우 batch size에 대해서도 평균을 해주면 된다.

<br>

**<span style='color:DarkRed'>Pros and Cons</span>**

- 장점
	- 어떤 input length에 대해서 학습이 가능
	- 위의 $\mathbf{W}_e, \mathbf{W}_h $는 매 step마다 같은 matrix로 sharing을 사용하기 때문에 모델의 사이즈가 input length에 의존적이지 않다. 
	- 이전 단계의 정보를 사용하여 계산하다.

- 단점
	- 계산이 느리다.
	- **먼 과거의 정보를 유지하기 힘들다.**


<br>

**<span style='color:DarkRed'>Reference</span>**

http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf

